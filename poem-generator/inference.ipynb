{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "csu-q0fiHKA-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu8Bjz2xm1cY"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth\n",
        "!pip install gdown\n",
        "\n",
        "import torch\n",
        "import gdown\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
        "from trl import SFTTrainer\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect drive"
      ],
      "metadata": {
        "id": "h39CZ7IWHTVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2ITkK7bqj4s",
        "outputId": "19abddd7-c173-4def-8498-9b0bc48956e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/vinallama_poem_model\"\n",
        "if os.path.exists(folder_path):\n",
        "    print(\"Thư mục tồn tại.\")\n",
        "else:\n",
        "    print(\"Thư mục chưa tồn tại.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltxj_u8yqyEb",
        "outputId": "51c45b48-31d1-4274-824a-996cdb7c8215"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thư mục tồn tại.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model and tokenizer"
      ],
      "metadata": {
        "id": "hFwfta_pHWJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/vinallama_poem_model\"\n",
        "\n",
        "max_seq_length = 4096\n",
        "dtype = \"float16\"\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=save_path,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {
        "id": "hj1-WoMRpknT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "k4lSvW7Kq-Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict function"
      ],
      "metadata": {
        "id": "Q-37cSxWHZ11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def generate_poem(prompt, max_new_tokens=200):\n",
        "    text = f\"\"\"<|im_start|>system\n",
        "Bạn là một AI thi sĩ chuyên sáng tác thơ lục bát bằng tiếng Việt.\n",
        "Hãy thể hiện cảm xúc tinh tế, sử dụng ngôn từ đẹp, và tuân thủ nghiêm ngặt các quy tắc sau:\n",
        "- Bài thơ có nhiều cặp dòng: một dòng 6 chữ, sau đó là một dòng 8 chữ.\n",
        "- Mỗi dòng phải xuống dòng rõ ràng.\n",
        "- Dòng 6 chữ có đúng 6 từ, dòng 8 chữ có đúng 8 từ.\n",
        "- Không được viết sai nhịp, không viết quá, viết thiếu số chữ.\n",
        "- Vần luật phải tự nhiên, đúng phong cách thơ ca truyền thống Việt Nam.<|im_end|>\n",
        "<|im_start|>user\n",
        "Hãy sáng tác một bài thơ lục bát về chủ đề '{prompt}'<|im_end|>\n",
        "\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length\n",
        "    )\n",
        "\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    generated_poem = generated_text.replace(text, \"\").strip()\n",
        "    return generated_poem\n"
      ],
      "metadata": {
        "id": "G-eZAAQTp0fU"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme = \"vầng trăng\""
      ],
      "metadata": {
        "id": "qEDGRV1QrLMM"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_poem = generate_poem(theme)"
      ],
      "metadata": {
        "id": "4ChXxMq_GXW-"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXTxSU7wGyoO",
        "outputId": "a328dbc7-1aa2-4a64-a9b0-29e1d9f74b1a"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|im_start|> system\n",
            "Bạn là một AI thi sĩ chuyên sáng tác thơ lục bát bằng tiếng Việt.\n",
            "Hãy thể hiện cảm xúc tinh tế, sử dụng ngôn từ đẹp, và tuân thủ nghiêm ngặt các quy tắc sau:\n",
            "- Bài thơ có nhiều cặp dòng: một dòng 6 chữ, sau đó là một dòng 8 chữ.\n",
            "- Mỗi dòng phải xuống dòng rõ ràng.\n",
            "- Dòng 6 chữ có đúng 6 từ, dòng 8 chữ có đúng 8 từ.\n",
            "- Không được viết sai nhịp, không viết quá, viết thiếu số chữ.\n",
            "- Vần luật phải tự nhiên, đúng phong cách thơ ca truyền thống Việt Nam. \n",
            "<|im_start|> user\n",
            "Hãy sáng tác một bài thơ lục bát về chủ đề 'vầng trăng' \n",
            "\n",
            "<|im_start|> assistant\n",
            "trăng ơi sao cứ lặng thinh\n",
            "để em nhung nhớ đêm trường cô đơn\n",
            "mây buồn phủ kín bầu trời\n",
            "giọt sương lạnh lẽo ướt rơi bên thềm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fastapi and ngronk"
      ],
      "metadata": {
        "id": "P0EHf_6PHf4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn\n"
      ],
      "metadata": {
        "id": "E9fb3YzBrZIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class Prompt(BaseModel):\n",
        "    title: str\n"
      ],
      "metadata": {
        "id": "9tcvez3c0FnA"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.post(\"/generate_poem\")\n",
        "def gen_poem(prompt: Prompt):\n",
        "    generated_poem = generate_poem(prompt)\n",
        "    generated_poem = generated_poem.split(\"assistant\")[-1].strip()\n",
        "    return {\"poem\": generated_poem}\n"
      ],
      "metadata": {
        "id": "F1_j80aj5c6Z"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import conf, ngrok\n",
        "\n",
        "nest_asyncio.apply()\n",
        "conf.get_default().auth_token = \"30QBZhUD0nbtwQptJyO8Vk1JKEW_5CtJidGwFCNFDN1B2YUKt\"\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH9CYA5G3jfq",
        "outputId": "9859155f-30dc-48ac-c4ba-91da146543f0"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://702899f907d7.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [752]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [752]\n"
          ]
        }
      ]
    }
  ]
}