{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b523e24dd1441ae6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install",
   "id": "77d895f56ebbbe7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth\n",
    "!pip install gdown\n",
    "\n",
    "import torch\n",
    "import gdown\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq, TextStreamer\n",
    "from trl import SFTTrainer\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "id": "86c6008889183d54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Connect drive",
   "id": "81d9b919a5b5d858"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "3b09937c7324a258"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load data",
   "id": "cfde14a6cdcdcba6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/content/dataset.csv\")\n",
    "\n",
    "df"
   ],
   "id": "d7e3e5ee420147e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split data",
   "id": "c0bf1db6d52764a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(df.head, test_size=0.1, random_state=42)\n",
    "\n",
    "dataset = {\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df),\n",
    "}\n",
    "\n",
    "print(f\"Training size: {len(dataset['train'])}, Validation size: {len(dataset['validation'])}\")"
   ],
   "id": "2bb38100468dea6c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
